{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMNyVV2KaI-P",
        "outputId": "d4cbff94-6d8a-4990-eb7f-f036e7e7c2b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "import re"
      ],
      "metadata": {
        "id": "RMSzaoYkhxRJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Done this task using Chat Completion API. The answer is based only the contexts we are providing. But the format of the citations are some what different."
      ],
      "metadata": {
        "id": "On4nLTJ6Z2LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up your OpenAI API credentials\n",
        "openai.api_key = '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'\n",
        "\n",
        "# Get user inputs\n",
        "question = input(\"Enter the question: \")\n",
        "num_contexts = int(input(\"Enter the number of context texts: \"))\n",
        "\n",
        "contexts = []\n",
        "authors = []\n",
        "\n",
        "# Get context texts and authors from user\n",
        "for i in range(num_contexts):\n",
        "    author = input(f\"Enter the author for context {i+1}: \")\n",
        "    text = input(f\"Enter the text for context {i+1}: \")\n",
        "    contexts.append(text)\n",
        "    authors.append(author)\n",
        "\n",
        "nxtline = \"\\n\"\n",
        "\n",
        "# Compose the zero-shot prompt\n",
        "message = f\"\"\"You are a student with no knowledge until fed. You have to answer the asked question only using the following authors and their texts. You are not supposed to answer anything other than using the following information.\n",
        "In a research use-case, citations are important for trust and verification. Provide an answer to the question below using the given contexts and include citations in paranthesis in the last of every line:{nxtline}\"\"\"\n",
        "for i in range(num_contexts):\n",
        "    message += f\"{authors[i]}: {contexts[i]}{nxtline}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": message},\n",
        "    {\"role\": \"user\", \"content\": question}\n",
        "]\n",
        "\n",
        "# Call the OpenAI API\n",
        "response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages = conversation,\n",
        "      temperature=0.7,\n",
        "      max_tokens=150,\n",
        "      #stop=None,\n",
        "      #echo=False,\n",
        "      n=1\n",
        "    )\n",
        "\n",
        "gpt_output = response.choices[0].message\n",
        "gpt_content_output = gpt_output[\"content\"]\n",
        "print(\"\\n\" ,gpt_content_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pZIsi74h6Vm",
        "outputId": "74cae992-accf-41c8-dd24-855843565bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the question: What is the difference between GPT and BERT models?\n",
            "Enter the number of context texts: 3\n",
            "Enter the author for context 1: Trinita Roy\n",
            "Enter the text for context 1: BERT is an encoder transformer model which is trained on two tasks - masked LM and next sentence prediction.\n",
            "Enter the author for context 2: Asheesh Kumar\n",
            "Enter the text for context 2: GPT is a decoder model which works best on sequence generation tasks\n",
            "Enter the author for context 3: Siddhant Jain\n",
            "Enter the text for context 3: LSTMs have been very popular for sequence to sequence tasks but have limitations in processing long texts.\n",
            "\n",
            " The main difference between GPT and BERT models lies in their architecture and the tasks they are trained on.\n",
            "\n",
            "GPT (Generative Pre-trained Transformer) is a decoder model that is specifically designed for sequence generation tasks. It excels in tasks such as language translation, text completion, and text generation. (Asheesh Kumar)\n",
            "\n",
            "On the other hand, BERT (Bidirectional Encoder Representations from Transformers) is an encoder transformer model. It is trained on two tasks - masked language modeling and next sentence prediction. BERT is known for its ability to understand the context and meaning of words within a sentence. It is widely used for tasks such as text classification, named entity recognition, and question answering. (Trinita Roy)\n",
            "\n",
            "In summary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Just the another way of using the Chat Completion API, added the input question in the prompt itself. Everything is right but the model is not providing the citations."
      ],
      "metadata": {
        "id": "swrJK-GOadfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "openai.api_key = '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'\n",
        "\n",
        "# user inputs\n",
        "question = input(\"Enter the question: \")\n",
        "num_contexts = int(input(\"Enter the number of context texts: \"))\n",
        "\n",
        "contexts = []\n",
        "authors = []\n",
        "\n",
        "# input context texts and authors\n",
        "for i in range(num_contexts):\n",
        "    author = input(f\"Enter the author for context {i+1}: \")\n",
        "    text = input(f\"Enter the text for context {i+1}: \")\n",
        "    contexts.append(text)\n",
        "    authors.append(author)\n",
        "\n",
        "nxtline = \"\\n\"\n",
        "\n",
        "# zero-shot prompt\n",
        "prompt = \"\"\n",
        "for i in range(num_contexts):\n",
        "    prompt += f\"{authors[i]}: {contexts[i]}{nxtline}\"\n",
        "\n",
        "message = f\"You are a student with no knowledge until fed. You have to answer the asked question only using the following authors and their texts. You are not supposed to answer anything other than using the following information:\\n\"\n",
        "message += prompt\n",
        "message += f\"\\nQuestion: {question}\\n\"\n",
        "\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": message},\n",
        "    {\"role\": \"user\", \"content\": question}\n",
        "]\n",
        "\n",
        "# call the API\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=conversation,\n",
        "    temperature=0.7,\n",
        "    max_tokens=100,\n",
        "    n=1\n",
        ")\n",
        "\n",
        "gpt_output = response.choices[0].message\n",
        "gpt_content_output = gpt_output[\"content\"]\n",
        "\n",
        " # extract the answer part\n",
        "\n",
        "answer = gpt_content_output.split('\\nQuestion:', 1)[0]\n",
        "formatted_answer = answer\n",
        "\n",
        "for i in range(num_contexts):\n",
        "    formatted_answer = formatted_answer.replace(contexts[i], f\"({authors[i]} et al.)\")\n",
        "\n",
        "print(formatted_answer)\n"
      ],
      "metadata": {
        "id": "DnMIJlG3QyL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7ee325-48cd-4ead-8ec9-661941cf8ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the question: What is the difference between GPT and BERT models?\\\n",
            "Enter the number of context texts: 3\n",
            "Enter the author for context 1: Trinita Roy\n",
            "Enter the text for context 1:  BERT is an encoder transformer model which is trained on two tasks - masked LM and next sentence prediction.\n",
            "Enter the author for context 2: Asheesh Kumar\n",
            "Enter the text for context 2: GPT is a decoder model which works best on sequence generation tasks\n",
            "Enter the author for context 3:  Siddhant Jain\n",
            "Enter the text for context 3: LSTMs have been very popular for sequence to sequence tasks but have limitations in processing long texts.\n",
            "The main difference between GPT and BERT models lies in their architecture and training tasks. GPT is a decoder model that excels in sequence generation tasks, while BERT is an encoder transformer model trained on masked LM and next sentence prediction tasks. Additionally, LSTMs have been popular for sequence to sequence tasks but have limitations in processing long texts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Here in the following code snippet, I've used the Completion API and the output answer is in the correct format as we want."
      ],
      "metadata": {
        "id": "PmeqaCzsbLaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = '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'\n",
        "\n",
        "# user inputs\n",
        "question = input(\"Enter the question: \")\n",
        "num_contexts = int(input(\"Enter the number of context texts: \"))\n",
        "\n",
        "contexts = []\n",
        "authors = []\n",
        "\n",
        "# input context texts and authors from user\n",
        "for i in range(num_contexts):\n",
        "    author = input(f\"Enter the author for context {i+1}: \")\n",
        "    text = input(f\"Enter the text for context {i+1}: \")\n",
        "    contexts.append(text)\n",
        "    authors.append(author)\n",
        "\n",
        "nxtline = \"\\n\"\n",
        "\n",
        "\n",
        "# zero-shot prompt\n",
        "prompt = f\"In a research use-case, citations are important for trust and verification. Provide an answer to the question below using the given contexts and include citations in parentheses at the end of every line:{nxtline}{nxtline}\"\n",
        "for i in range(num_contexts):\n",
        "    prompt += f\"{authors[i]}: {contexts[i]} ({authors[i]} et al.){nxtline}\"\n",
        "\n",
        "prompt += f\"{nxtline}Question: {question}\"\n",
        "\n",
        "# the API call\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    max_tokens=150,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "answer = response.choices[0].text.strip()\n",
        "\n",
        "print(\"\\n\",answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfa4AwipXtoN",
        "outputId": "732f3eec-614b-4caa-804a-fc5d82a1bdc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the question: What is the difference between GPT and BERT models?\n",
            "Enter the number of context texts: 3\n",
            "Enter the author for context 1: Trinita Roy\n",
            "Enter the text for context 1:  BERT is an encoder transformer model which is trained on two tasks - masked LM and next sentence prediction.\n",
            "Enter the author for context 2: Asheesh Kumar\n",
            "Enter the text for context 2: GPT is a decoder model which works best on sequence generation tasks\n",
            "Enter the author for context 3:  Siddhant Jain\n",
            "Enter the text for context 3: LSTMs have been very popular for sequence to sequence tasks but have limitations in processing long texts.\n",
            "\n",
            " The primary difference between GPT and BERT models is that GPT is a decoder model that works best on sequence generation tasks, while BERT is an encoder transformer model that is trained on two tasks - masked LM and next sentence prediction (Trinita Roy et al.). Unlike BERT, LSTMs are more popular for sequence to sequence tasks, but have limitations in processing long texts (Siddhant Jain et al.).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. For the following code, I tried making an extra function for adding citations. This code also uses Chat Completion API."
      ],
      "metadata": {
        "id": "G4ZsMIE7oj1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = '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'\n",
        "\n",
        "nxtline = \"\\n\"\n",
        "\n",
        "\n",
        "# function to sdd citations\n",
        "def add_citations(response, contexts, authors):\n",
        "    lines = response.split(nxtline)\n",
        "    cited_lines = []\n",
        "    for line in lines:\n",
        "        cited_line = line\n",
        "        for i in range(len(contexts)):\n",
        "            if contexts[i] in line:\n",
        "                citation = f\"({authors[i]}, et al.)\"\n",
        "                cited_line = cited_line.replace(contexts[i], f\"{contexts[i]} {citation}\")\n",
        "        cited_lines.append(cited_line)\n",
        "    return '\\n'.join(cited_lines)\n",
        "\n",
        "# user inputs\n",
        "question = input(\"Enter the question: \")\n",
        "num_contexts = int(input(\"Enter the number of context texts: \"))\n",
        "\n",
        "contexts = []\n",
        "authors = []\n",
        "\n",
        "# context texts and authors from user\n",
        "for i in range(num_contexts):\n",
        "    author = input(f\"Enter the author for context {i+1}: \")\n",
        "    text = input(f\"Enter the text for context {i+1}: \")\n",
        "    contexts.append(text)\n",
        "    authors.append(author)\n",
        "\n",
        "# zero-shot prompt\n",
        "message = f\"You are a student with no knowledge until fed. You have to answer the asked question only using the following authors and their texts. You are not supposed to answer anything other than using the following information:{nxtline}\"\n",
        "for i in range(num_contexts):\n",
        "    message += f\"{authors[i]}: {contexts[i]}{nxtline}\"\n",
        "\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": message},\n",
        "    {\"role\": \"user\", \"content\": question}\n",
        "]\n",
        "\n",
        "# call the API\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=conversation,\n",
        "    temperature=0.7,\n",
        "    max_tokens=100,\n",
        "    n=1\n",
        ")\n",
        "\n",
        "gpt_output = response.choices[0].message\n",
        "gpt_content_output = gpt_output[\"content\"]\n",
        "\n",
        "# add citations\n",
        "response_with_citations = add_citations(gpt_content_output, contexts, authors)\n",
        "print(\"\\n\", response_with_citations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqZ-tMc3je86",
        "outputId": "d4e2cdad-4aaa-4b2e-bf43-755ed56e172a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the question: What is the difference between GPT and BERT models?\n",
            "Enter the number of context texts: 3\n",
            "Enter the author for context 1: Trinita Roy\n",
            "Enter the text for context 1:  BERT is an encoder transformer model which is trained on two tasks - masked LM and next sentence prediction.\n",
            "Enter the author for context 2: Asheesh Kumar\n",
            "Enter the text for context 2: GPT is a decoder model which works best on sequence generation tasks\n",
            "Enter the author for context 3:  Siddhant Jain\n",
            "Enter the text for context 3: LSTMs have been very popular for sequence to sequence tasks but have limitations in processing long texts.\n",
            "\n",
            " The difference between GPT and BERT models lies in their architecture and training objectives. \n",
            "\n",
            "GPT, as mentioned by Asheesh Kumar, is a decoder model that excels in sequence generation tasks. It is trained to predict the next word in a sequence given the previous context. GPT is designed to generate coherent and contextually appropriate text.\n",
            "\n",
            "On the other hand, BERT, as discussed by Trinita Roy, is an encoder transformer model. It is trained on two tasks - masked language\n"
          ]
        }
      ]
    }
  ]
}